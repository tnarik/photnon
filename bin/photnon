#!/usr/bin/env python3

from photnon.data_extraction import extract_data
from photnon.data_analysis import bsize_value, report_dupes, generate_dupes_info, produce_dupes_script, REMOVAL_CODE_SCHEDULE, REMOVAL_CODE_IGNORE, PERSIST_VERSION_KEEP, read_datafiles

import os
import sys
import tables.scripts.ptrepack as ptrepack

import re

from prompt_toolkit.shortcuts import confirm
from prompt_toolkit import prompt
from prompt_toolkit.completion import WordCompleter
from prompt_toolkit.validation import Validator, ValidationError

import time
import pandas as pd
import numpy as np
from datetime import datetime as dt

import tempfile

import argparse

from colorama import init, Fore
init(autoreset=True)

EXIT_CODE_WORKINGINFO_MISMATCH = 100
EXIT_CODE_NO_SINGLE_OUTPUT = 101

def all_folders(folders, result = set()):
  if isinstance(folders, np.ndarray):
    folders = folders.tolist()
  if not isinstance(folders, list):
    folders = [folders]

  for folder in folders:
    result.add(folder)
    head, tail = os.path.split(folder)
    if head and tail:
      all_folders(head, result)
  return result

def preferred_folder(full_folders):
  folders = sorted(all_folders(full_folders))
  print("\n".join(folders[0:10]))
  return
  folder_completer = WordCompleter(folders)

  validator = Validator.from_callable(
    lambda x: x in folders,
    error_message='not a valid folder',
    move_cursor_to_end=True)
  preferred_folder = prompt('Enter the preferred folder (use [TAB]): ', completer=folder_completer,
    validator=validator)
  print(preferred_folder)


if __name__ == "__main__":
  # working_info adds information about the data extraction process (as the files were last accessed in that context)
  working_info = { 'wd': [os.getcwd()],
                   'hostname': [os.uname()[1]] }
  parser = argparse.ArgumentParser(description="Photon")
  parser.add_argument('-d', '--data',
            nargs='+',
            help='data files. if -s is used, only the first datafile will be taken into account',
            dest='datafiles')
  parser.add_argument('-o', '--output',
            nargs='?',
            const='',
            help='generate output datafile. if no file is indicated, the input one will overwriten (if there is just one)')
  parser.add_argument('-v', '--verbose',
            help='verbose output',
            action='count',
            default=0)
  parser.add_argument('-f', '--force',
            help='force reprocessing of all entries of a datafile/recreation in the case of extraction',
            action='store_true')
  parser.add_argument('-r', '--repack_off',
            help='disable repack output datafile after processing',
            action='store_false',
            dest='repack')
  parser.add_argument('-s', '--space',
            nargs='+',
            help='files, folders or pattern space to explore',
            dest='space')

  # test
  parser.add_argument('-t',
    dest='preffold_test',
    action='store_true')

  args = parser.parse_args()

  if args.preffold_test:
    folds = np.load("folds.npy")
    preferred_folder(folds)
    exit(2)

  data = None
  if args.datafiles:
    print("specified datafile{} '{}{}{}'\n".format("s" if len(args.datafiles)>1 else "", Fore.GREEN, ' / '.join(args.datafiles), Fore.RESET))
  else:
    print('NO datafiles specified')


  if args.space:
    extract_data(args.space, args.datafiles[0] if args.datafiles else None, verbose=args.verbose, working_info=working_info, force=args.force)
  else:
    if not args.datafiles:
      parser.print_help()
    else:
      computed_columns = ['mtime_date', 'datetime_date'] # Values that cannot be stored as HDF and are computable
      divergent_columns = ['atime', 'ctime', 'should_remove', 'persist_version'] # Values which might differ without impacting file identity (some are computed)

      ph_working_info, ph_ok_orig, ph_error_orig, num_read_ok, num_read_error = read_datafiles(working_info, args.datafiles, deduplicate=True)

      if len(ph_working_info) < len(args.datafiles) or ph_working_info.hostname.nunique() > 1:
        print(ph_working_info)
        print("{}Datafiles generated on different (or unknown) systems".format(Fore.RED, Fore.RESET))
        print("{}Final scripts would run on a single machine, which could cause problems in some scenarios (see README.md).{}".format(Fore.YELLOW, Fore.RESET))
        if not confirm(
              suffix="(y/N)",
              message="Do you want to proceed?"):
          sys.exit(EXIT_CODE_WORKINGINFO_MISMATCH)

      print("total        : {} (ok: {}/ error: {})".format(num_read_ok + num_read_error, num_read_ok, num_read_error))
      print("deduplicated : {} (ok: {}/ error: {})".format(len(ph_ok_orig) + len(ph_error_orig), len(ph_ok_orig), len(ph_error_orig)))

      ph_ok = ph_ok_orig
      ph_error = ph_error_orig
      if not args.force:
        if 'should_remove' in ph_ok_orig.columns:
          ph_ok = ph_ok_orig.loc[ph_ok_orig[ph_ok_orig.should_remove != REMOVAL_CODE_SCHEDULE].index]
        if 'should_remove' in ph_error_orig.columns:
          ph_error = ph_error_orig.loc[ph_error_orig[ph_error_orig.should_remove != REMOVAL_CODE_SCHEDULE].index]

      num_ok = len(ph_ok)
      num_error = len(ph_error)
      num_total = num_ok + num_error
      print("processing   : {} (ok: {}/ error: {})".format(num_total, num_ok, num_error))
      print("{}================================".format(Fore.YELLOW))
      print("{}% errors{}: {:.2%}".format(Fore.GREEN, Fore.RESET, num_error/ num_total))

      ph_ok['mtime_date'] = ph_ok.mtime.apply(lambda x: x.date())
      ph_ok['datetime_date'] = ph_ok.datetime.apply(lambda x: x.date())
      ph_ok['second_discrepancy'] = (ph_ok.datetime - ph_ok.mtime).apply(lambda x: abs(x.total_seconds()))

      print("{}% matching times{}: {:.2%}".format(Fore.GREEN,Fore.RESET,
                sum(ph_ok.mtime == ph_ok.datetime)))#.value_counts(normalize=True, sort=False)[True]))

      print("{}% matching dates{}: {:.2%}".format(Fore.GREEN,Fore.RESET,
                sum(ph_ok.mtime_date == ph_ok.datetime_date)))#.value_counts(normalize=True)[True]))

      print("{0}% with discrepancy{1}:{0} 1 minute:{1} {2:.2%} {0}/ 1 hour:{1} {3:.2%} {0}/ 1 day:{1} {4:.2%}".format(Fore.GREEN,Fore.RESET,
                sum(ph_ok.second_discrepancy <= 60)/len(ph_ok),
                sum(ph_ok.second_discrepancy <= 3600)/len(ph_ok),
                sum(ph_ok.second_discrepancy <= 24*3600)/len(ph_ok)))

      print("{}% timeless{}: {:.2%}".format(Fore.GREEN,Fore.RESET,
                len(ph_ok[ph_ok.timeless])/len(ph_ok)))
      if len(ph_ok[ph_ok.timeless]) > 0:
        print("{0}% with discrepancy (timeless) {1}:{0} 1 minute:{1} {2:.2%} {0}/ 1 hour:{1} {3:.2%} {0}/ 1 day:{1} {4:.2%}".format(Fore.GREEN,Fore.RESET,
                  sum(ph_ok[ph_ok.timeless].second_discrepancy <= 60)/len(ph_ok[ph_ok.timeless]),
                  sum(ph_ok[ph_ok.timeless].second_discrepancy <= 3600)/len(ph_ok[ph_ok.timeless]),
                  sum(ph_ok[ph_ok.timeless].second_discrepancy <= 24*3600)/len(ph_ok[ph_ok.timeless])))

      for label, photos_df in [("OK", ph_ok), ("ERROR", ph_error)]:
        print("{}================================ {} set".format(Fore.YELLOW, label))
        # All duplicates
        dup_full = photos_df.duplicated(keep=False, subset=photos_df.columns[1:].drop(divergent_columns, errors='ignore'))
        dup_full_except_first = photos_df.duplicated(keep='first', subset=photos_df.columns[1:].drop(divergent_columns, errors='ignore'))
        # Not used, but I was logging that before
        dup_full_reduced = dup_full ^ dup_full_except_first

        #if len(photos_df[dup_full]) > 0:
        #  print("{}- in folders:\n{}{}".format(Fore.GREEN,Fore.RESET,
        #            (photos_df[dup_full].folder.value_counts()).to_string()))
  
        # Digest duplicates, computed differently. Clearly slower and more complex (more lines) but it doesn't matter that much
        list_digest = photos_df.digest.value_counts()
        dup_digest_values = list_digest[list_digest > 1].index.values
        dup_digest = photos_df.digest.isin(dup_digest_values)
        dup_digest_except_first = photos_df.duplicated(keep='first', subset=['digest'])

        #if len(photos_df[dup_digest]) > 0:
        #  print("{}- in folders:\n{}{}".format(Fore.GREEN,Fore.RESET,
        #          (photos_df[dup_digest].folder.value_counts()).to_string()))
        print("photos {}, after reducing:".format(
          len(photos_df)
          ))
        print("   - full   -> {} (removing {} and processing {})".format(
          sum(~dup_full_except_first),
          sum(dup_full_except_first),
          sum(dup_full)
          ))
        print("   - digest -> {} (removing {} and processing {})".format(
          sum(~dup_digest_except_first),
          sum(dup_digest_except_first),
          sum(dup_digest)
          ))

        if 'size' in photos_df:
          print("size {:.3f} {}, after reducing:".format(
            *bsize_value(photos_df['size'].sum())
            ))
          print("   - full   -> {:.3f} {} (removing {:.3f} {} and processing {:.3f} {})".format(
            *bsize_value(photos_df[~dup_full_except_first]['size'].sum()),
            *bsize_value(photos_df[dup_full_except_first]['size'].sum()),
            *bsize_value(photos_df[dup_full]['size'].sum())
            ))
          print("   - digest -> {:.3f} {} (removing {:.3f} {} and processing {:.3f} {})".format(
            *bsize_value(photos_df[~dup_digest_except_first]['size'].sum()),
            *bsize_value(photos_df[dup_digest_except_first]['size'].sum()),
            *bsize_value(photos_df[dup_digest]['size'].sum())
            ))
        print()

        result = True
        '''confirm(
              suffix="(y/N)",
              message="Do you want to process OK duplicates?")'''
  
        if result:
          #if confirm(
          #      suffix="(y/N)",
          #      message="Generate simple HTML for inspection?"):
          #  print("HTML will be generated")
          #  str = "<html><body>"
          #  for i, p in photos_df[dup_full].sort_values(by=['name', 'folder']).iterrows():#.path.values:
          #    src = os.path.join(p['folder'], p['name'])
          #    str+="<div>{}<img src='{}' width='30%'/></div>".format(src, src)
          #  str +="</body></html>"
          #  with open('inspection_OK.html', 'w') as f:
          #    f.write(str)
  
          # Keep everything by default
          photos_df.loc[:, 'should_remove'] = REMOVAL_CODE_IGNORE
          photos_df.loc[:, 'persist_version'] = PERSIST_VERSION_KEEP
  
          #preferred_folder(photos_df.loc[dup_full].folder.unique())

          print("{}   - full -{}".format(Fore.GREEN,Fore.RESET))
          generate_dupes_info(photos_df, dup_full, verbose = args.verbose)
          report_dupes(photos_df, dup_full, sum(dup_digest_except_first), verbose = args.verbose)
  
          print("{}   - digest -{}".format(Fore.GREEN,Fore.RESET))
          generate_dupes_info(photos_df, dup_digest, verbose = args.verbose)
          report_dupes(photos_df, dup_digest, sum(dup_digest_except_first), verbose = args.verbose)
          produce_dupes_script(photos_df, dup_digest, "dup_actions_{}.sh".format(label))

      # ALL sets processed
      if args.output is not None:
        if len(args.output) > 0:
          print("use '{}'".format(args.output))
          datafilename = "{}.pho".format(args.output)
        elif len(args.datafiles) == 1:
          print("use '{}'".format(args.datafiles[0]))
          datafilename = "{}.pho".format(args.datafiles[0])
        else:
          print("There are several input datafiles and no single output")
          sys.exit(EXIT_CODE_NO_SINGLE_OUTPUT)

        ph_ok.drop(computed_columns, axis=1).to_hdf(datafilename, key='ok', format="table")
        ph_error.to_hdf(datafilename, key='error', format="table")
        # After reading the datafiles, the working information needs to be sanitised and refreshed
        pd.DataFrame( { 'wd': ['.'],
                        'hostname': [os.uname()[1]] }
                    ).to_hdf(datafilename, key='info', format="table")

        if args.repack:
          with tempfile.TemporaryDirectory() as tempdir:
            sys.argv = ['ptrepack', datafilename, os.path.join(tempdir,'repackedfile')]
            ptrepack.main()
            if False:
              os.rename(datafilename, "{}_back".format(datafilename))
            os.rename(os.path.join(tempdir,'repackedfile'), datafilename)

