#!/usr/bin/env python3

from photnon.data_extraction import extract_data
from photnon.data_analysis import preduplication_info, general_info, timed_info
from photnon.data_analysis import deduplication_process, read_datafiles, produce_retime_script, enrich
from photnon.data_analysis import REMOVAL_CODE_SCHEDULE, REMOVAL_CODE_IGNORE, PERSIST_VERSION_KEEP
from photnon import storage

import os
import sys
import tables.scripts.ptrepack as ptrepack

import re

from prompt_toolkit.shortcuts import confirm
from prompt_toolkit import prompt
from prompt_toolkit.completion import WordCompleter
from prompt_toolkit.validation import Validator, ValidationError

import time
import pandas as pd
import numpy as np
from datetime import datetime as dt

import tempfile

import argparse

from colorama import init, Fore
init(autoreset=True)



import timeit





EXIT_CODE_WORKINGINFO_MISMATCH = 100
EXIT_CODE_NO_SINGLE_OUTPUT = 101
EXIT_CODE_NO_COMMANDS = 102

def all_folders_with_counts(folders):
  folder_counts = folders.value_counts()
  folder_tokens = [folder_count.split(os.path.sep) for folder_count in folder_counts.index.values]
  folder_tokens_counts = pd.concat([pd.DataFrame(folder_tokens, index=folder_counts.index), folder_counts.to_frame()], axis=1)

  file_counts = pd.Series()
  last_level_counts = None
  for top in range(1, len(folder_tokens_counts.columns)-1):
    positions = list(folder_tokens_counts.columns[:-top].values)
    folder_tokens_counts_aggregated = folder_tokens_counts.loc[~folder_tokens_counts[positions[-1]].isna()].fillna('').groupby(positions, axis=0).sum()

    # This is a pretty simplistic stopping clause, because it assumes the bottom is reached at the same time
    if last_level_counts is not None:
      for new_level_index in folder_tokens_counts_aggregated.index:
        # If a given new aggregated index has a single child...
        if new_level_index in last_level_counts.index:
          if len(last_level_counts.loc[new_level_index, :]) == 1:
            # Then we find and remove entries on the master count, to stop counting those
            folder_tokens_counts_aggregated.drop(new_level_index, axis=0, inplace=True)
            subqueries = []
            for i in positions:
              subqueries.append("(@new_level_index[{0}] == @folder_tokens_counts[{0}])".format(i))
            query = " & ".join(subqueries)
            folder_tokens_counts.drop(folder_tokens_counts.query(query).index, inplace=True)

      # if all entries in the new count are parents of previous ones, and they don't add anything,
      # they'll be gone at this point.
      if len(folder_tokens_counts_aggregated) == 0:
        break
    last_level_counts = folder_tokens_counts_aggregated.copy()
    file_counts = file_counts.append(folder_tokens_counts_aggregated['folder'])

  return file_counts.sort_values(ascending=False)

def select_preferred_folder(full_folders):
  NONE_PREFERRED = '[NONE]'

  folders = all_folders_with_counts(full_folders).index.map(lambda ix: os.path.sep.join(ix))
  folders = folders.insert(0, NONE_PREFERRED)
  folder_completer = WordCompleter(folders)

  validator = Validator.from_callable(
    lambda x: x in folders,
    error_message='not a valid folder',
    move_cursor_to_end=True)
  preferred_folder = prompt('Enter the preferred folder (use [TAB]): ', completer=folder_completer,
    validator=validator)

  if preferred_folder == NONE_PREFERRED: preferred_folder = False
  return preferred_folder


if __name__ == "__main__":
  # working_info adds information about the data extraction process (as the files were last accessed in that context)
  working_info = { 'wd': [os.getcwd()],
                   'hostname': [os.uname()[1]] }
  parser = argparse.ArgumentParser(description="Photon", prefix_chars="-+")
  parser.add_argument('-d', '--data',
            nargs='+',
            help='data files. if -s is used, only the first datafile will be taken into account',
            dest='datafiles')
  parser.add_argument('-o', '--output',
            nargs='?',
            const='',
            help='generate output datafile. if no file is indicated, the input one will overwriten (if there is just one)')
  parser.add_argument('-v', '--verbose',
            help='verbose output',
            action='count',
            default=0)
  parser.add_argument('-f', '--force',
            help='force reprocessing of all entries of a datafile/recreation in the case of extraction',
            action='store_true')
  parser.add_argument('-r', '--repack_off',
            help='disable repack output datafile after processing',
            action='store_false',
            dest='repack')
  parser.add_argument('-s', '--space',
            nargs='+',
            help='files, folders or pattern space to explore',
            dest='space')
  parser.add_argument('-p', '--preferred',
            help='preferred folder to consider when processing duplicates (otherwise it will be asked interactively)',
            nargs='?',
            default=False,
            dest='preferred_folder')
  parser.add_argument('-l', '--list',
            help='list information about the datafile',
            action='store_true',
            dest='list')
  parser.add_argument('-t',
            help='test enrich',
            action='store_true',
            dest='test')

  args = parser.parse_args()

  data = None
  if args.datafiles:
    print("specified datafile{} '{}{}{}'\n".format("s" if len(args.datafiles)>1 else "", Fore.GREEN, ' / '.join(args.datafiles), Fore.RESET))
  else:
    print('NO datafiles specified')


  # First step is reading files
  if args.space:
    extract_data(args.space, args.datafiles[0] if args.datafiles else None, working_info=working_info, verbose=args.verbose, force=args.force)
    exit()

  # If we are not reading files, then we should be reading data
  if not args.datafiles:
    parser.print_help()
    exit(EXIT_CODE_NO_COMMANDS)



  #computed_columns = ['mtime_date', 'datetime_date', 'folder_date'] # Values that cannot be stored as HDF and are computable
  divergent_columns = ['atime', 'ctime', 'should_remove', 'persist_version'] # Values which might differ without impacting file identity (some are computed)

  ph_working_info, ph_ok_orig, ph_error_orig, num_read_ok, num_read_error = read_datafiles(working_info, args.datafiles, deduplicate=True)
  if args.list:
    print(ph_working_info)
    print(ph_ok_orig.columns)
    exit()

  if len(ph_working_info) < len(args.datafiles) or ph_working_info.hostname.nunique() > 1:
    print(ph_working_info)
    print("{}Datafiles generated on different (or unknown) systems".format(Fore.RED, Fore.RESET))
    print("{}Final scripts would run on a single machine, which could cause problems in some scenarios (see README.md).{}".format(Fore.YELLOW, Fore.RESET))
    print("{}The output file will be marked as generated on the first machine.{}".format(Fore.YELLOW, Fore.RESET))
    if not confirm(
          suffix="(y/N)",
          message="Do you want to proceed?"):
      exit(EXIT_CODE_WORKINGINFO_MISMATCH)
    ph_working_info.hostname = [ph_working_info.hostname[0]]

  print("original     : {} (ok: {}/ error: {})".format(num_read_ok + num_read_error, num_read_ok, num_read_error))
  print("post-drop    : {} (ok: {}/ error: {})".format(len(ph_ok_orig) + len(ph_error_orig), len(ph_ok_orig), len(ph_error_orig)))

  ph_ok = ph_ok_orig
  ph_error = ph_error_orig
  if not args.force:
    if 'should_remove' in ph_ok_orig.columns:
      ph_ok = ph_ok_orig.loc[ph_ok_orig[ph_ok_orig.should_remove != REMOVAL_CODE_SCHEDULE].index]
    if 'should_remove' in ph_error_orig.columns:
      ph_error = ph_error_orig.loc[ph_error_orig[ph_error_orig.should_remove != REMOVAL_CODE_SCHEDULE].index]

  ## Below some commented out timing code for the enrich process
  #import timeit
  #t = timeit.timeit(lambda : enrich(ph_ok), number=200)
  #print(t/200)
  #exit(2)
  computed_columns = enrich(ph_ok)
  if args.test:
    print("===",args.preferred_folder,'===')
    print(ph_ok.columns)
    print(ph_ok[ph_ok.folder_date.notna()].folder_date.unique())
    print(ph_ok[ph_ok.folder_month_date.notna()].folder_month_date.unique())
    print(ph_ok.dtypes)
    #print(ph_ok[ph_ok.folder_has_date != '']['folder_has_date'])
    exit(2)

  general_info(ph_ok, ph_error)
  timed_info(ph_ok)

  preferred_folder = args.preferred_folder
  if args.preferred_folder != False:
    if args.preferred_folder is None:
      print("{}Calculating default folder priority, to display options by number of files".format(Fore.GREEN))
      preferred_folder = select_preferred_folder(ph_ok_orig['folder'].append(ph_error_orig['folder']))

  for label, photos_df in [("OK", ph_ok), ("ERROR", ph_error)]:
    print("{}================================ {} set".format(Fore.YELLOW, label))
    # All duplicates
    dup_full = photos_df.duplicated(keep=False, subset=photos_df.columns[1:].drop(divergent_columns, errors='ignore'))
    dup_full_except_first = photos_df.duplicated(keep='first', subset=photos_df.columns[1:].drop(divergent_columns, errors='ignore'))

    # Digest duplicates, computed differently. Clearly slower and more complex (more lines) but it doesn't matter that much
    list_digest = photos_df.digest.value_counts()
    dup_digest = photos_df.digest.isin(list_digest[list_digest > 1].index.values)
    dup_digest_except_first = photos_df.duplicated(keep='first', subset=['digest'])

    preduplication_info(photos_df, dup_full, dup_full_except_first, dup_digest, dup_digest_except_first)
    deduplication_process(photos_df, dup_full, dup_digest, "dup_actions_{}.sh".format(label),
            label = label,
            working_info = ph_working_info,
            preferred_folder=preferred_folder,
            goal=sum(dup_digest_except_first), verbose = args.verbose)


  print("\n{}================================ after deduplication".format(Fore.YELLOW))
  general_info(ph_ok.loc[ph_ok[ph_ok.should_remove != REMOVAL_CODE_SCHEDULE].index], ph_error.loc[ph_error[ph_error.should_remove != REMOVAL_CODE_SCHEDULE].index])
  timed_info(ph_ok.loc[ph_ok[ph_ok.should_remove != REMOVAL_CODE_SCHEDULE].index])

  t = timeit.timeit(lambda : produce_retime_script(ph_ok, script="retime.sh"), number=1)
  print(t/1)
  #produce_retime_script(ph_ok, script="retime.sh")

  print("\n{}================================ after retiming".format(Fore.YELLOW))
  retimeable_photos = ph_ok[ph_ok['should_remove'] != REMOVAL_CODE_SCHEDULE].copy()
  #for i, p in retimeable_photos.iterrows():
  #  p['mtime'] = p['datetime']#time.mktime(time.strptime(p['datetime'], '%Y-%m-%d %H:%M:%S'));
  retimeable_photos['mtime'] = retimeable_photos['datetime']
  general_info(ph_ok.loc[ph_ok[ph_ok.should_remove != REMOVAL_CODE_SCHEDULE].index], ph_error.loc[ph_error[ph_error.should_remove != REMOVAL_CODE_SCHEDULE].index])
  timed_info(retimeable_photos)#ph_ok.loc[ph_ok[ph_ok.should_remove != REMOVAL_CODE_SCHEDULE].index])


  # ALL sets processed
  if args.output is not None:
    if len(args.output) > 0:
      print("use '{}'".format(args.output))
      datafilename = storage.normalize(args.output)
    elif len(args.datafiles) == 1:
      print("use '{}'".format(args.datafiles[0]))
      datafilename = storage.normalize(args.datafiles[0])
    else:
      print("There are several input datafiles and no single output")
      exit(EXIT_CODE_NO_SINGLE_OUTPUT)

    ph_ok.drop(computed_columns, axis=1).to_hdf(datafilename, key='ok', format="table")
    ph_error.to_hdf(datafilename, key='error', format="table")
    # After reading the datafiles, the working information needs to be sanitised and refreshed
    pd.DataFrame( { 'wd': ['.'],
                    'hostname': ph_working_info.hostname }
                ).to_hdf(datafilename, key='info', format="table")

    if args.repack:
      with tempfile.TemporaryDirectory() as tempdir:
        sys.argv = ['ptrepack', datafilename, os.path.join(tempdir,'repackedfile')]
        ptrepack.main()
        # Enable below line if you want to keep a backup of the previous HDF5 datafile
        if False:
          os.rename(datafilename, "{}_back".format(datafilename))
        os.rename(os.path.join(tempdir,'repackedfile'), datafilename)
